{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum CNN\n",
    "\n",
    "- Since quantum models can only do binary classification, we modify the MNIST dataset, in order to train the model. We create a \"conv-pool-conv-pool-conv-pool\" (similar to LeNet) model, which extracts the features necessary to train the model. The official tutorial from Qiskit can be viewed [here](https://qiskit-community.github.io/qiskit-machine-learning/tutorials/11_quantum_convolutional_neural_networks.html).\n",
    "\n",
    "### Workflow for QCNN\n",
    "\n",
    "- One of the major disadvantages of current (quantum) models is that they can only accept a small number of qubits (inputs). That said, it is physically impossible to train on QCNN using actual images. A pretty simple workaround for this problem is to use an autoencoder to reduce the dimensionality of the image, so that it can be passed to the model. This is exactly what we did and it worked pretty well. We used a simple autoencoder, which enabled us to project the 28x28 images down to a vector of only 8 values, which were then fed to the model. From here, we were able to train the model and test its accuracy.\n",
    "\n",
    "The pipeline is as follows:\n",
    "\n",
    "$$\n",
    "image \\rightarrow autoencoder \\rightarrow QCNN\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # used for optimization libraries (SGD, Adam, etc)\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # used for progress bars\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import ZFeatureMap\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_globals.random_seed = 17\n",
    "torch.manual_seed(17)  # computers a (pseudo) random, so specifying a seed allows for reproducibility\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 1e-3\n",
    "nb_channels = 16\n",
    "embedding_dim = 8\n",
    "batch_size = 64\n",
    "nb_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "\n",
    "def plot_imgs(input, output):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(input, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(output, cmap=\"gray\")\n",
    "    ax[1].set_title(\"Output\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_encoded_decoded(model, input):\n",
    "    z = model.encode(input)\n",
    "    output = model.decode(z)\n",
    "    return z, output\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_loader, optimizer, nb_epochs):\n",
    "    for epoch in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "            # send the data to cuda, if possible\n",
    "            data = data.to(DEVICE)\n",
    "            output = model(data)\n",
    "            loss = 0.5 * (output - data).pow(2).sum() / data.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {acc_loss:.4f}\")\n",
    "\n",
    "\n",
    "def train_binary_classifier(model, data_loader, num_epochs, learning_rate):\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train the network\n",
    "    print(\"Training...\\n\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(data_loader)):\n",
    "            # send the data to cuda, if possible\n",
    "            data = data.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # forward prop\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets.to(torch.float32).unsqueeze(1))\n",
    "\n",
    "            # backward prop\n",
    "            optimizer.zero_grad()  # MAKE SURE TO RESET THE GRADIENTS\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"Done training, thankfully!\\n\")\n",
    "\n",
    "\n",
    "def check_accuracy(loader: DataLoader, model, train: bool = True):\n",
    "    \"\"\"\n",
    "    Function to check the accuracy of the trained model\n",
    "\n",
    "    :param loader: the datasets on which the model will be evaluated on\n",
    "    :param model: the model that you would like to evaluate\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        print(\"Checking accuracy on training data\\n\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\\n\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            scores = model(x)\n",
    "\n",
    "            predictions = torch.where(scores > 0.5, 1, 0).flatten()\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy of {float(num_correct)/float(num_samples) * 100:.2f}%\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCNN Utils\n",
    "\n",
    "\n",
    "def conv_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(np.pi / 2, 0)\n",
    "    return target\n",
    "\n",
    "\n",
    "def conv_layer(num_qubits, param_prefix):\n",
    "    qc = QuantumCircuit(num_qubits, name=\"Convolutional Layer\")\n",
    "    qubits = list(range(num_qubits))\n",
    "    param_index = 0\n",
    "    params = ParameterVector(param_prefix, length=num_qubits * 3)\n",
    "    for q1, q2 in zip(qubits[0::2], qubits[1::2]):\n",
    "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "    for q1, q2 in zip(qubits[1::2], qubits[2::2] + [0]):\n",
    "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.append(qc_inst, qubits)\n",
    "    return qc\n",
    "\n",
    "\n",
    "def pool_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "    return target\n",
    "\n",
    "\n",
    "def pool_layer(sources, sinks, param_prefix):\n",
    "    num_qubits = len(sources) + len(sinks)\n",
    "    qc = QuantumCircuit(num_qubits, name=\"Pooling Layer\")\n",
    "    param_index = 0\n",
    "    params = ParameterVector(param_prefix, length=num_qubits // 2 * 3)\n",
    "    for source, sink in zip(sources, sinks):\n",
    "        qc = qc.compose(pool_circuit(params[param_index : (param_index + 3)]), [source, sink])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.append(qc_inst, range(num_qubits))\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to reduce the size of the image, so instead of cropping we can use an autoencoder to reduce the dimensionality\n",
    "# note: PCA is a linear autoencoder\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels: int, reduction: int = 16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if len(x.size()) == 3:\n",
    "            b, c, _ = x.size()\n",
    "        else:\n",
    "            b, c, _, _ = x.size()\n",
    "        avg_pool = self.avg_pool(x).view(b, c)\n",
    "        max_pool = self.max_pool(x).view(b, c)\n",
    "        return self.sigmoid(self.fc(avg_pool) + self.fc(max_pool)).view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels: int, reduction: int = 16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel_attn = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attn = SpatialAttention()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.channel_attn(x) * x\n",
    "        x = self.spatial_attn(x) * x\n",
    "        return x\n",
    "\n",
    "\n",
    "# CBAM paper - https://arxiv.org/abs/1807.06521\n",
    "# CBAM is a module that uses both channel and spatial attention to improve the performance of a model\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, nb_channels: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, nb_channels, kernel_size=5),  # to 24x24 - we are using MNIST\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=5),  # to 20x20\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # to 9x9\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # to 4x4\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, embedding_dim, kernel_size=4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, nb_channels, kernel_size=4),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # from 4x4\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # from 9x9\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=5),  # from 20x20\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, 1, kernel_size=5),  # from 24x24\n",
    "        )\n",
    "\n",
    "        # print the number of parameters\n",
    "        print(f\"number of parameters: {(sum(p.numel() for p in self.parameters())):,}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x).view(x.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z.view(z.size(0), -1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Why put the CBAM module directly after the conv layer?\n",
    "  - well, from theory, we see that the typical conv-batchnorm-relu is a good combination so it makes sense to put the CBAM module after the conv layer.\n",
    "    this block (conv-batchnorm-relu) make sense, because relu clips the values so you are essnetially losing some info. we would ideally want to learn\n",
    "    what to pay \"attention\" to before we clip the values. that said, we're also swapping relu for gelu because it has been shown to perform better (it does in this). \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AutoEncoderCBAM(nn.Module):\n",
    "    def __init__(self, nb_channels: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, nb_channels, kernel_size=5),  # to 24x24 - we are using MNIST\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=5),  # to 20x20\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # to 9x9\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # to 4x4\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, embedding_dim, kernel_size=4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, nb_channels, kernel_size=4),\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # from 4x4\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # from 9x9\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=5),  # from 20x20\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, 1, kernel_size=5),  # from 24x24\n",
    "        )\n",
    "\n",
    "        # print the number of parameters\n",
    "        print(f\"number of parameters: {(sum(p.numel() for p in self.parameters())):,}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x).view(x.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z.view(z.size(0), -1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_set = datasets.MNIST(\"data/mnist/\", train=True, download=True)\n",
    "ae_train_dataset = ae_train_set.data.view(-1, 1, 28, 28).float()\n",
    "mu, std = ae_train_dataset.mean(), ae_train_dataset.std()\n",
    "ae_train_dataset.sub_(mu).div_(std)\n",
    "ae_train_loader = DataLoader(dataset=ae_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ae_test_set = datasets.MNIST(\"data/mnist/\", train=False, download=True)\n",
    "ae_test_dataset = ae_test_set.data.view(-1, 1, 28, 28).float()\n",
    "ae_test_dataset.sub_(mu).div_(std)\n",
    "None  # prevent it from printing the last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(nb_channels=nb_channels, embedding_dim=embedding_dim)\n",
    "autoencoder = autoencoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_cbam = AutoEncoderCBAM(nb_channels=nb_channels, embedding_dim=embedding_dim)\n",
    "autoencoder_cbam = autoencoder_cbam.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_optimizer = lambda model: optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "ae_optimizer = get_optimizer(autoencoder)\n",
    "cbam_ae_optimizer = get_optimizer(autoencoder_cbam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_autoencoder(autoencoder, ae_train_loader, ae_optimizer, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_autoencoder(autoencoder_cbam, ae_train_loader, cbam_ae_optimizer, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ae_test_dataset[56].to(DEVICE)\n",
    "\n",
    "# Encode / decode\n",
    "z, output = get_encoded_decoded(autoencoder, input)\n",
    "z_cbam, output_cbam = get_encoded_decoded(autoencoder_cbam, input.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(input.cpu().detach().numpy().squeeze(), output.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(input.cpu().detach().numpy().squeeze(), output_cbam.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qcnn_circuit(num_qubits: int = 8) -> tuple[QuantumCircuit, EstimatorQNN]:\n",
    "    assert num_qubits <= 12, \"We need to set an artificial limit on the number of qubits, so choose a value <= 12\"\n",
    "    feature_map = ZFeatureMap(feature_dimension=num_qubits)\n",
    "    ansatz = QuantumCircuit(num_qubits, name=\"ansatz\")\n",
    "\n",
    "    ansatz.compose(conv_layer(num_qubits=num_qubits, param_prefix=\"c1\"), qubits=list(range(num_qubits)), inplace=True)\n",
    "    ansatz.compose(\n",
    "        pool_layer(\n",
    "            sources=list(range(num_qubits // 2)), sinks=list(range(num_qubits // 2, num_qubits)), param_prefix=\"p1\"\n",
    "        ),\n",
    "        qubits=list(range(num_qubits)),\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    ansatz.compose(\n",
    "        conv_layer(num_qubits=num_qubits // 2, param_prefix=\"c2\"),\n",
    "        qubits=list(range(num_qubits // 2, num_qubits)),\n",
    "        inplace=True,\n",
    "    )\n",
    "    ansatz.compose(\n",
    "        pool_layer(\n",
    "            sources=list(range(num_qubits // 4)), sinks=list(range(num_qubits // 4, num_qubits // 2)), param_prefix=\"p2\"\n",
    "        ),\n",
    "        qubits=list(range(num_qubits // 2, N)),\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    ansatz.compose(\n",
    "        conv_layer(num_qubits=num_qubits // 4, param_prefix=\"c3\"),\n",
    "        qubits=list(range(num_qubits - (num_qubits // 4), N)),\n",
    "        inplace=True,\n",
    "    )\n",
    "    ansatz.compose(\n",
    "        pool_layer(sources=list(range(2)), sinks=list(range(2, num_qubits // 4)), param_prefix=\"p3\"),\n",
    "        qubits=list(range(num_qubits - (num_qubits // 4), num_qubits)),\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Combining the feature map and ansatz\n",
    "    circuit = QuantumCircuit(num_qubits)\n",
    "    circuit.compose(feature_map, range(num_qubits), inplace=True)\n",
    "    circuit.compose(ansatz, range(num_qubits), inplace=True)\n",
    "\n",
    "    observable = SparsePauliOp.from_list([(\"Z\" + \"I\" * (num_qubits - 1), 1)])\n",
    "\n",
    "    # we decompose the circuit for the QNN to avoid additional data copying\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=circuit.decompose(),\n",
    "        observables=observable,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "    )\n",
    "\n",
    "    return circuit, qnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "circuit, qcnn = get_qcnn_circuit(num_qubits=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.draw(\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes too long to train QCNN\n",
    "\n",
    "qcnn_train_set = datasets.MNIST(\"data/mnist/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "qcnn_test_set = datasets.MNIST(\"data/mnist/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    qcnn_train_set.data,\n",
    "    qcnn_test_set.data,\n",
    "    qcnn_train_set.targets,\n",
    "    qcnn_test_set.targets,\n",
    ")\n",
    "\n",
    "\n",
    "def prep_dataset(X, y, filter_vals, dataset_size):\n",
    "    X = X.unsqueeze(1).reshape(-1, 1, 28, 28).to(torch.float32)\n",
    "    X_filtered = np.concatenate((X[filter_vals[0]][: dataset_size // 2], X[filter_vals[1]][: dataset_size // 2]))\n",
    "    y_filtered = np.concatenate((y[filter_vals[0]][: dataset_size // 2], y[filter_vals[1]][: dataset_size // 2]))\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "\n",
    "def change_labels(labels, set_1, set_2, set_1_val, set_2_val):\n",
    "    labels[labels == set_1] = set_1_val\n",
    "    labels[labels == set_2] = set_2_val\n",
    "    return labels\n",
    "\n",
    "\n",
    "N = 100\n",
    "set_1 = 3\n",
    "set_1_vals_train = np.where(y_train == set_1)[0]\n",
    "set_1_vals_test = np.where(y_test == set_1)[0]\n",
    "set_2 = 6\n",
    "set_2_vals_train = np.where(y_train == set_2)[0]\n",
    "set_2_vals_test = np.where(y_test == set_2)[0]\n",
    "\n",
    "encode = lambda x: autoencoder_cbam.encode(x.to(DEVICE)).detach().cpu().numpy()\n",
    "\n",
    "X_train, y_train = prep_dataset(X_train, y_train, [set_1_vals_train, set_2_vals_train], N)\n",
    "X_test, y_test = prep_dataset(X_test, y_test, [set_1_vals_test, set_2_vals_test], N)\n",
    "\n",
    "\n",
    "X_train_encoded = encode(torch.Tensor(X_train))\n",
    "X_test_encoded = encode(torch.Tensor(X_test))\n",
    "\n",
    "X_train_encoded, y_train = shuffle(X_train_encoded, y_train, random_state=17)\n",
    "X_test_encoded, y_test = shuffle(X_test_encoded, y_test, random_state=17)\n",
    "\n",
    "\n",
    "# labels/targets need to be 1 and -1\n",
    "y_train = change_labels(y_train, set_1, set_2, -1, 1)\n",
    "y_test = change_labels(y_test, set_1, set_2, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetworkClassifier(\n",
    "    qcnn,\n",
    "    optimizer=COBYLA(maxiter=200),  # Set max iterations here\n",
    "    callback=callback_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "x = np.asarray(X_train_encoded)\n",
    "y = np.asarray(y_train)\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(X_test_encoded)\n",
    "y = np.asarray(y_test)\n",
    "print(f\"Accuracy from the test data : {np.round(100 * classifier.score(x, y), 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binary classfication, we need the labels to be either 0 or 1\n",
    "y_train = change_labels(y_train, 1, -1, 1, 0)\n",
    "y_test = change_labels(y_test, 1, -1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet paper: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 2):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(2, 2),\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=16,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=120,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        self.linear1 = nn.Linear(120, 84)\n",
    "        self.linear2 = nn.Linear(84, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, X_test, y_train, y_test, batch_size: int = 64):\n",
    "    train_loader = DataLoader(\n",
    "        [(X_train[i], y_train[i]) for i in range(len(X_train))], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader([(X_test[i], y_test[i]) for i in range(len(X_test))], batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_dataloaders(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_classes = 1\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "# init network\n",
    "model = LeNet(num_classes=num_classes)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary_classifier(model, train_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classical model performed better, while less than 400x the time to train. This is sort of disingenious, because we trained this network with an entire image, whereas the QCNN used an encode version of the image, which only has 8 values. That said, let's train a network that is more comparable to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(8, 32, 1)\n",
    "mlp = mlp.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_loader, encoded_test_loader = get_dataloaders(X_train_encoded, X_test_encoded, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary_classifier(mlp, train_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(train_loader, mlp)\n",
    "check_accuracy(test_loader, mlp, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get even better results (which is to be expected). We can see that a QCNN is not suited for a binary classification task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

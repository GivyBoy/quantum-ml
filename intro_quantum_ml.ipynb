{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Intro to Quantum ML\n",
    "\n",
    "- The idea is to explore the potential applications of quantum computing in machine learning\n",
    "\n",
    "### Model\n",
    "\n",
    "- We'll be looking at using a QCNN (Quantum CNN) for classifying MNIST images. This is a fairly simple dataset, which makes it perfect for to test the efficacy of the Quantum Algorithm. In order to compare the model's performance, we will compare it to [LeNet-5](\"http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\"). This model was released in 1998 by Yann LeCun. It was one of the earliest convnets used for image recognition, so it seems reasonable to consider it as a baseline for comparison to the QCNN, which is still in its infancy. `[we can't do this comparison yet]`\n",
    "\n",
    "- Additionally, we will compare both a Quantum and Classical Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # used for optimization libraries (SGD, Adam, etc)\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # used for progress bars\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(17)  # computers a (pseudo) random, so specifying a seed allows for reproducibility\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Steps (for classical models)\n",
    "\n",
    "- Define hyperparams and util functions\n",
    "- Create models\n",
    "- Create dataloaders\n",
    "- Train models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 1e-3\n",
    "nb_channels = 16\n",
    "embedding_dim = 8\n",
    "batch_size = 64\n",
    "nb_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def plot_imgs(input, output):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(input, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(output, cmap=\"gray\")\n",
    "    ax[1].set_title(\"Output\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_encoded_decoded(model, input):\n",
    "    z = model.encode(input)\n",
    "    output = model.decode(z)\n",
    "    return z, output\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_loader, optimizer, nb_epochs):\n",
    "    for epoch in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "            # send the data to cuda, if possible\n",
    "            data = data.to(DEVICE)\n",
    "            output = model(data)\n",
    "            loss = 0.5 * (output - data).pow(2).sum() / data.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {acc_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=16,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=120,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        self.linear1 = nn.Linear(120, 84)\n",
    "        self.linear2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to reduce the size of the image, so instead of cropping we can use an autoencoder ro reduce the dimensionality\n",
    "# note: PCA is a linear autoencoder\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels: int, reduction: int = 16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if len(x.size()) == 3:\n",
    "            b, c, _ = x.size()\n",
    "        else:\n",
    "            b, c, _, _ = x.size()\n",
    "        avg_pool = self.avg_pool(x).view(b, c)\n",
    "        max_pool = self.max_pool(x).view(b, c)\n",
    "        return self.sigmoid(self.fc(avg_pool) + self.fc(max_pool)).view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels: int, reduction: int = 16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel_attn = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attn = SpatialAttention()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.channel_attn(x) * x\n",
    "        x = self.spatial_attn(x) * x\n",
    "        return x\n",
    "\n",
    "\n",
    "# CBAM paper - https://arxiv.org/abs/1807.06521\n",
    "# CBAM is a module that uses both channel and spatial attention to improve the performance of a model\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, nb_channels: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, nb_channels, kernel_size=5),  # to 24x24 - we are using MNIST\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=5),  # to 20x20\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # to 9x9\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # to 4x4\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, embedding_dim, kernel_size=4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, nb_channels, kernel_size=4),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # from 4x4\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # from 9x9\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=5),  # from 20x20\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, 1, kernel_size=5),  # from 24x24\n",
    "        )\n",
    "\n",
    "        # print the number of parameters\n",
    "        print(f\"number of parameters: {(sum(p.numel() for p in self.parameters())):,}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x).view(x.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z.view(z.size(0), -1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Why put the CBAM module directly after the conv layer?\n",
    "  - well, from theory, we see that the typical conv-batchnorm-relu is a good combination so it makes sense to put the CBAM module after the conv layer.\n",
    "    this block (conv-batchnorm-relu) make sense, because relu clips the values so you are essnetially losing some info. we would ideally want to learn\n",
    "    what to pay \"attention\" to before we clip the values. that said, we're also swapping relu for gelu because it has been shown to perform better (it does in this). \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AutoEncoderCBAM(nn.Module):\n",
    "    def __init__(self, nb_channels: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, nb_channels, kernel_size=5),  # to 24x24 - we are using MNIST\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=5),  # to 20x20\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # to 9x9\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # to 4x4\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, embedding_dim, kernel_size=4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, nb_channels, kernel_size=4),\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=3, stride=2),  # from 4x4\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=4, stride=2),  # from 9x9\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=5),  # from 20x20\n",
    "            CBAM(nb_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, 1, kernel_size=5),  # from 24x24\n",
    "        )\n",
    "\n",
    "        # print the number of parameters\n",
    "        print(f\"number of parameters: {(sum(p.numel() for p in self.parameters())):,}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x).view(x.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z.view(z.size(0), -1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_set = datasets.MNIST(\"data/mnist/\", train=True, download=True)\n",
    "ae_train_dataset = ae_train_set.data.view(-1, 1, 28, 28).float()\n",
    "mu, std = ae_train_dataset.mean(), ae_train_dataset.std()\n",
    "ae_train_dataset.sub_(mu).div_(std)\n",
    "ae_train_loader = DataLoader(dataset=ae_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ae_test_set = datasets.MNIST(\"data/mnist/\", train=False, download=True)\n",
    "ae_test_dataset = ae_test_set.data.view(-1, 1, 28, 28).float()\n",
    "ae_test_dataset.sub_(mu).div_(std)\n",
    "None  # prevent it from printing the last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(nb_channels=nb_channels, embedding_dim=embedding_dim)\n",
    "autoencoder = autoencoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_cbam = AutoEncoderCBAM(nb_channels=nb_channels, embedding_dim=embedding_dim)\n",
    "autoencoder_cbam = autoencoder_cbam.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_optimizer = lambda model: optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "ae_optimizer = get_optimizer(autoencoder)\n",
    "cbam_ae_optimizer = get_optimizer(autoencoder_cbam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_autoencoder(autoencoder, ae_train_loader, ae_optimizer, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_autoencoder(autoencoder_cbam, ae_train_loader, cbam_ae_optimizer, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ae_test_dataset[56].to(DEVICE)\n",
    "\n",
    "# Encode / decode\n",
    "z, output = get_encoded_decoded(autoencoder, input)\n",
    "z_cbam, output_cbam = get_encoded_decoded(autoencoder_cbam, input.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(input.cpu().detach().numpy().squeeze(), output.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(input.cpu().detach().numpy().squeeze(), output_cbam.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import ZFeatureMap\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "algorithm_globals.random_seed = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(np.pi / 2, 0)\n",
    "    return target\n",
    "\n",
    "\n",
    "# Let's draw this circuit and see what it looks like\n",
    "params = ParameterVector(\"θ\", length=3)\n",
    "circuit = conv_circuit(params)\n",
    "circuit.draw(\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(num_qubits, param_prefix):\n",
    "    qc = QuantumCircuit(num_qubits, name=\"Convolutional Layer\")\n",
    "    qubits = list(range(num_qubits))\n",
    "    param_index = 0\n",
    "    params = ParameterVector(param_prefix, length=num_qubits * 3)\n",
    "    for q1, q2 in zip(qubits[0::2], qubits[1::2]):\n",
    "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "    for q1, q2 in zip(qubits[1::2], qubits[2::2] + [0]):\n",
    "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.append(qc_inst, qubits)\n",
    "    return qc\n",
    "\n",
    "\n",
    "circuit = conv_layer(4, \"θ\")\n",
    "circuit.decompose().draw(\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "params = ParameterVector(\"θ\", length=3)\n",
    "circuit = pool_circuit(params)\n",
    "circuit.draw(\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_layer(sources, sinks, param_prefix):\n",
    "    num_qubits = len(sources) + len(sinks)\n",
    "    qc = QuantumCircuit(num_qubits, name=\"Pooling Layer\")\n",
    "    param_index = 0\n",
    "    params = ParameterVector(param_prefix, length=num_qubits // 2 * 3)\n",
    "    for source, sink in zip(sources, sinks):\n",
    "        qc = qc.compose(pool_circuit(params[param_index : (param_index + 3)]), [source, sink])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.append(qc_inst, range(num_qubits))\n",
    "    return qc\n",
    "\n",
    "\n",
    "sources = [0, 1]\n",
    "sinks = [2, 3]\n",
    "circuit = pool_layer(sources, sinks, \"θ\")\n",
    "circuit.decompose().draw(\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes too long to train QCNN\n",
    "\n",
    "# qcnn_train_set = datasets.MNIST(\"data/mnist/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "# qcnn_test_set = datasets.MNIST(\"data/mnist/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = (\n",
    "#     qcnn_train_set.data,\n",
    "#     qcnn_test_set.data,\n",
    "#     qcnn_train_set.targets,\n",
    "#     qcnn_test_set.targets,\n",
    "# )\n",
    "\n",
    "# X_train = X_train.unsqueeze(1).reshape(-1, 1, 28, 28).to(torch.float32)\n",
    "# X_test = X_test.unsqueeze(1).reshape(-1, 1, 28, 28).to(torch.float32)\n",
    "\n",
    "# X_train_encoded = autoencoder.encode(X_train.to(DEVICE)).detach().cpu().numpy()\n",
    "# X_test_encoded = autoencoder.encode(X_test.to(DEVICE)).detach().cpu().numpy()\n",
    "# y_train = y_train.numpy()\n",
    "# y_test = y_test.numpy()\n",
    "\n",
    "# X_train_encoded = np.concatenate(\n",
    "#     (X_train_encoded[np.where(y_train == 3)[0]], X_train_encoded[np.where(y_train == 6)[0]])\n",
    "# )\n",
    "# y_train = np.concatenate((y_train[np.where(y_train == 3)[0]], y_train[np.where(y_train == 6)[0]]))\n",
    "\n",
    "# X_test_encoded = np.concatenate((X_test_encoded[np.where(y_test == 3)[0]], X_test_encoded[np.where(y_test == 6)[0]]))\n",
    "# y_test = np.concatenate((y_test[np.where(y_test == 3)[0]], y_test[np.where(y_test == 6)[0]]))\n",
    "\n",
    "# X_train_encoded, y_train = shuffle(X_train_encoded, y_train, random_state=17)\n",
    "# X_test_encoded, y_test = shuffle(X_test_encoded, y_test, random_state=17)\n",
    "\n",
    "# # labels/targets need to be 1 and -1\n",
    "# y_train[y_train == 3] = -1\n",
    "# y_train[y_train == 6] = 1\n",
    "# y_test[y_test == 3] = -1\n",
    "# y_test[y_test == 6] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_images):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hor_array = np.zeros((6, 8))\n",
    "    ver_array = np.zeros((4, 8))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, 7):\n",
    "        if i != 3:\n",
    "            hor_array[j][i] = np.pi / 2\n",
    "            hor_array[j][i + 1] = np.pi / 2\n",
    "            j += 1\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, 4):\n",
    "        ver_array[j][i] = np.pi / 2\n",
    "        ver_array[j][i + 4] = np.pi / 2\n",
    "        j += 1\n",
    "\n",
    "    for n in range(num_images):\n",
    "        rng = algorithm_globals.random.integers(0, 2)\n",
    "        if rng == 0:\n",
    "            labels.append(-1)\n",
    "            random_image = algorithm_globals.random.integers(0, 6)\n",
    "            images.append(np.array(hor_array[random_image]))\n",
    "        elif rng == 1:\n",
    "            labels.append(1)\n",
    "            random_image = algorithm_globals.random.integers(0, 4)\n",
    "            images.append(np.array(ver_array[random_image]))\n",
    "\n",
    "        # Create noise\n",
    "        for i in range(8):\n",
    "            if images[-1][i] == 0:\n",
    "                images[-1][i] = algorithm_globals.random.uniform(0, np.pi / 4)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(100)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = ZFeatureMap(8)\n",
    "\n",
    "ansatz = QuantumCircuit(8, name=\"Ansatz\")\n",
    "\n",
    "# First Convolutional Layer\n",
    "ansatz.compose(conv_layer(8, \"c1\"), list(range(8)), inplace=True)\n",
    "\n",
    "# First Pooling Layer\n",
    "ansatz.compose(pool_layer([0, 1, 2, 3], [4, 5, 6, 7], \"p1\"), list(range(8)), inplace=True)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "ansatz.compose(conv_layer(4, \"c2\"), list(range(4, 8)), inplace=True)\n",
    "\n",
    "# Second Pooling Layer\n",
    "ansatz.compose(pool_layer([0, 1], [2, 3], \"p2\"), list(range(4, 8)), inplace=True)\n",
    "\n",
    "# Third Convolutional Layer\n",
    "ansatz.compose(conv_layer(2, \"c3\"), list(range(6, 8)), inplace=True)\n",
    "\n",
    "# Third Pooling Layer\n",
    "ansatz.compose(pool_layer([0], [1], \"p3\"), list(range(6, 8)), inplace=True)\n",
    "\n",
    "# Combining the feature map and ansatz\n",
    "circuit = QuantumCircuit(8)\n",
    "circuit.compose(feature_map, range(8), inplace=True)\n",
    "circuit.compose(ansatz, range(8), inplace=True)\n",
    "\n",
    "observable = SparsePauliOp.from_list([(\"Z\" + \"I\" * 7, 1)])\n",
    "\n",
    "# we decompose the circuit for the QNN to avoid additional data copying\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=circuit.decompose(),\n",
    "    observables=observable,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=ansatz.parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.draw(\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetworkClassifier(\n",
    "    qnn,\n",
    "    optimizer=COBYLA(maxiter=200),  # Set max iterations here\n",
    "    callback=callback_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "# classifier.fit(X_train_encoded, y_train)\n",
    "\n",
    "x = np.asarray(train_images)\n",
    "y = np.asarray(train_labels)\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(test_images)\n",
    "y = np.asarray(test_labels)\n",
    "print(f\"Accuracy from the test data : {np.round(100 * classifier.score(x, y), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from qiskit import ClassicalRegister, QuantumRegister\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "\n",
    "algorithm_globals.random_seed = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansatz(num_qubits):\n",
    "    return RealAmplitudes(num_qubits, reps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_encoder_circuit(num_latent, num_trash):\n",
    "    qr = QuantumRegister(num_latent + 2 * num_trash + 1, \"q\")\n",
    "    cr = ClassicalRegister(1, \"c\")\n",
    "    circuit = QuantumCircuit(qr, cr)\n",
    "    circuit.compose(ansatz(num_latent + num_trash), range(0, num_latent + num_trash), inplace=True)\n",
    "    circuit.barrier()\n",
    "    auxiliary_qubit = num_latent + 2 * num_trash\n",
    "    # swap test\n",
    "    circuit.h(auxiliary_qubit)\n",
    "    for i in range(num_trash):\n",
    "        circuit.cswap(auxiliary_qubit, num_latent + i, num_latent + num_trash + i)\n",
    "\n",
    "    circuit.h(auxiliary_qubit)\n",
    "    circuit.measure(auxiliary_qubit, cr[0])\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_idx(j, i):\n",
    "    # Index for zero pixels\n",
    "    return [\n",
    "        [i, j],\n",
    "        [i - 1, j - 1],\n",
    "        [i - 1, j + 1],\n",
    "        [i - 2, j - 1],\n",
    "        [i - 2, j + 1],\n",
    "        [i - 3, j - 1],\n",
    "        [i - 3, j + 1],\n",
    "        [i - 4, j - 1],\n",
    "        [i - 4, j + 1],\n",
    "        [i - 5, j],\n",
    "    ]\n",
    "\n",
    "\n",
    "def one_idx(i, j):\n",
    "    # Index for one pixels\n",
    "    return [[i, j - 1], [i, j - 2], [i, j - 3], [i, j - 4], [i, j - 5], [i - 1, j - 4], [i, j]]\n",
    "\n",
    "\n",
    "def get_dataset_digits(num, draw=True):\n",
    "    # Create Dataset containing zero and one\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for i in range(int(num / 2)):\n",
    "        # First we introduce background noise\n",
    "        empty = np.array([algorithm_globals.random.uniform(0, 0.1) for i in range(32)]).reshape(8, 4)\n",
    "\n",
    "        # Now we insert the pixels for the one\n",
    "        for i, j in one_idx(2, 6):\n",
    "            empty[j][i] = algorithm_globals.random.uniform(0.9, 1)\n",
    "        train_images.append(empty)\n",
    "        train_labels.append(1)\n",
    "        if draw:\n",
    "            plt.title(\"This is a One\")\n",
    "            plt.imshow(train_images[-1])\n",
    "            plt.show()\n",
    "\n",
    "    for i in range(int(num / 2)):\n",
    "        empty = np.array([algorithm_globals.random.uniform(0, 0.1) for i in range(32)]).reshape(8, 4)\n",
    "\n",
    "        # Now we insert the pixels for the zero\n",
    "        for k, j in zero_idx(2, 6):\n",
    "            empty[k][j] = algorithm_globals.random.uniform(0.9, 1)\n",
    "\n",
    "        train_images.append(empty)\n",
    "        train_labels.append(0)\n",
    "        if draw:\n",
    "            plt.imshow(train_images[-1])\n",
    "            plt.title(\"This is a Zero\")\n",
    "            plt.show()\n",
    "\n",
    "    train_images = np.array(train_images)\n",
    "    train_images = train_images.reshape(len(train_images), 32)\n",
    "\n",
    "    for i in range(len(train_images)):\n",
    "        sum_sq = np.sum(train_images[i] ** 2)\n",
    "        train_images[i] = train_images[i] / np.sqrt(sum_sq)\n",
    "\n",
    "    return train_images, train_labels\n",
    "\n",
    "\n",
    "train_images, __ = get_dataset_digits(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent = 3\n",
    "num_trash = 2\n",
    "\n",
    "fm = RawFeatureVector(2 ** (num_latent + num_trash))\n",
    "\n",
    "ae = auto_encoder_circuit(num_latent, num_trash)\n",
    "\n",
    "qc = QuantumCircuit(num_latent + 2 * num_trash + 1, 1)\n",
    "qc = qc.compose(fm, range(num_latent + num_trash))\n",
    "qc = qc.compose(ae)\n",
    "\n",
    "qc.draw(output=\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_interpret(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "qnn = SamplerQNN(\n",
    "    circuit=qc,\n",
    "    input_params=fm.parameters,\n",
    "    weight_params=ae.parameters,\n",
    "    interpret=identity_interpret,\n",
    "    output_shape=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func_digits(params_values):\n",
    "    probabilities = qnn.forward(train_images, params_values)\n",
    "    cost = np.sum(probabilities[:, 1]) / train_images.shape[0]\n",
    "\n",
    "    # plotting part\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(cost)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "    plt.show()\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "opt = COBYLA(maxiter=150)\n",
    "\n",
    "objective_func_vals = []\n",
    "# make the plot nicer\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "start = time.time()\n",
    "opt_result = opt.minimize(\n",
    "    fun=cost_func_digits, x0=np.random.random(30)\n",
    ")  # x0 - initilize the model at a random 'location'\n",
    "# because the inherent stochasticity with the (pseudo) random number (array in this case) generator, the model's performance will vary greatly\n",
    "elapsed = time.time() - start\n",
    "print(f\"Fit in {elapsed:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_qc = QuantumCircuit(num_latent + num_trash)\n",
    "test_qc = test_qc.compose(fm)\n",
    "ansatz_qc = ansatz(num_latent + num_trash)\n",
    "test_qc = test_qc.compose(ansatz_qc)\n",
    "test_qc.barrier()\n",
    "test_qc.reset(4)\n",
    "test_qc.reset(3)\n",
    "test_qc.barrier()\n",
    "test_qc = test_qc.compose(ansatz_qc.inverse())\n",
    "\n",
    "# sample new images\n",
    "test_images, test_labels = get_dataset_digits(2, draw=False)\n",
    "for image, label in zip(test_images, test_labels):\n",
    "    original_qc = fm.assign_parameters(image)\n",
    "    original_sv = Statevector(original_qc).data\n",
    "    original_sv = np.reshape(np.abs(original_sv) ** 2, (8, 4))\n",
    "\n",
    "    param_values = np.concatenate((image, opt_result.x))\n",
    "    output_qc = test_qc.assign_parameters(param_values)\n",
    "    output_sv = Statevector(output_qc).data\n",
    "    output_sv = np.reshape(np.abs(output_sv) ** 2, (8, 4))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(original_sv, cmap=\"gray\")\n",
    "    ax1.set_title(\"Input Data\")\n",
    "    ax2.imshow(output_sv, cmap=\"gray\")\n",
    "    ax2.set_title(\"Output Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_images = torch.Tensor(\n",
    "    np.array(\n",
    "        [\n",
    "            nn.functional.pad(torch.tensor(train_image), (1, 3)).reshape(6, 6).unsqueeze(0)\n",
    "            for train_image in train_images\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, std = padded_train_images.mean(), padded_train_images.std()\n",
    "padded_train_images.sub_(mu).div_(std)\n",
    "padded_train_images_loader = DataLoader(dataset=padded_train_images, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallAutoEncoder(nn.Module):\n",
    "    def __init__(self, nb_channels: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, nb_channels, kernel_size=3, padding=1),  # to 6x6\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, padding=1),  # to 6x6\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, padding=1),  # to 6x6\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3),  # to 4x4\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(nb_channels, embedding_dim, kernel_size=4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, nb_channels, kernel_size=4),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=3),  # from 4x4\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=1, stride=1),  # from 6x6\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=1, stride=1),  # from 6x6\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(nb_channels, 1, kernel_size=1, stride=1),  # from 6x6\n",
    "        )\n",
    "\n",
    "        # print the number of parameters\n",
    "        print(f\"number of parameters: {(sum(p.numel() for p in self.parameters())):,}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x).view(x.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z.view(z.size(0), -1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_autoencoder = SmallAutoEncoder(nb_channels=1, embedding_dim=5)\n",
    "small_autoencoder = small_autoencoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_autoencoder(small_autoencoder, padded_train_images_loader, get_optimizer(small_autoencoder), nb_epochs=1_000)\n",
    "elapsed = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fit in {elapsed:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = padded_train_images[7].to(DEVICE)\n",
    "z, output = get_encoded_decoded(small_autoencoder, input)\n",
    "plot_imgs(input.cpu().detach().numpy().squeeze(), output.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Classical Autoencoder performs pretty well and takes half the time to achieve similar/better results than the Quantum Autoencoder. It is also important to note that the Classical Autoencoder is a very simple model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
